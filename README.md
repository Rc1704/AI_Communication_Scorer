AI Communication Scorer

An AI-powered tool that automatically evaluates a student's self-introduction speech based on a rubric with communication skills framework.

This project combines:

Rule-based checks (keywords, salutation, flow)

Statistical features (TTR, WPM, filler word rate)

Sentiment analysis (engagement)

Semantic similarity (SBERT sentence embeddings)

A clean Streamlit web interface

It produces a 0â€“100 score with detailed breakdowns and feedback. 


AI_Communication_Scorer
â”‚
â”œâ”€â”€ app.py                # Streamlit frontend UI
â”œâ”€â”€ scoring.py            # Core scoring engine (Rubric + NLP + heuristics)
â”œâ”€â”€ text_utils.py         # Preprocessing, tokenization, keyword detection
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ Sample text for case study.txt   # Example transcript (optional) 


ğŸš€ Features 

âœ” 1. Transcript Input

Paste any self-introduction speech text and enter the duration (seconds).


âœ” 2. Rule-Based Content Evaluation

Checks for:

Name

Age

School/Class

Family

Interests/Hobbies 

Good-to-have elements like plans, achievement, fun facts

Each contributes to Content & Structure score (0â€“40).


âœ” 3. Speech Rate Scoring

Computes WPM (Words Per Minute) and maps to:

Ideal (111â€“140 WPM)

Slightly slow/fast

Too slow/too fast


âœ” 4. Language & Grammar

Instead of using external grammar APIs (which fail due to Java/API limits), grammar is scored using a lightweight heuristic, checking:

Misuse of lowercase â€œ i â€

Double spaces

Strange tokens with numbers & letters

Basic structural errors

Vocabulary richness is computed using Type-Token Ratio (TTR). 


âœ” 5. Clarity

Penalizes excessive filler words like:

um, uh, like, actually, basically, you know, kinda, i mean, well, hmm


âœ” 6. Engagement

Uses VADER sentiment analysis to detect positivity / enthusiasm.


âœ” 7. Semantic Similarity (NLP)

Each transcript is compared with 4 "ideal rubric descriptions":

Content & Structure

Language

Clarity

Engagement

Using sentence-transformers (all-MiniLM-L6-v2):

similarity = cos_sim(embedding(transcript), embedding(ideal_description))

These values (0â€“1) appear in the UI for interpretability.


âœ” 8. Feedback Summary

Strengths (top 2 scoring areas) 

Areas for improvement (bottom 2 scoring areas)

Missing content elements 


ğŸ“Š Scoring Formula (0â€“100)

1. Content & Structure â€“ 40 Points

| Component                | Points |
| ------------------------ | ------ |
| Salutation               | 0â€“5    |
| Keyword coverage         | 0â€“30   |
| Flow / logical structure | 0â€“5    | 


2. Speech Rate â€“ 10 Points

Based on WPM:

Ideal (111â€“140): 10

Slightly slow/fast: 6

Too slow/too fast: 2


3. Language & Grammar â€“ 20 Points

Grammar heuristic â†’ 0â€“10

Vocabulary richness (TTR) â†’ 0â€“10


4. Clarity â€“ 15 Points

Penalty for high filler word rate. 


5. Engagement â€“ 15 Points

Sentiment positivity (VADER). 


Total = 40 + 10 + 20 + 15 + 15 = 100

ğŸ”® Future Enhancements

Add live speech input (ASR â†’ transcript â†’ scoring)

Add multilingual scoring

Direct grammar API integration (paid or self-hosted LanguageTool)

Teacher dashboard for batch scoring

Real-time scoring during practice sessions

ğŸ“ Author

Developed as part of the Nirmaan Foundation AI Case Study.

